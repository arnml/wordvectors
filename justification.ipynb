{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Word Vectors\n",
        "## NLP with Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Natural Language Processing Tasks\n",
        "- Machine translation\n",
        "- Summarization and analysis of text\n",
        "- Question answering and information retrieval\n",
        "- Speech-to-text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Word Representation\n",
        "- Signifier\n",
        "- Sign\n",
        "- Signified (Meaning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Timeline of Word Representations\n",
        "| Year  | Representation Type         |\n",
        "|-------|-----------------------------|\n",
        "| 2000s | LSA and LDA                 |\n",
        "| 2013  | Word2Vec                    |\n",
        "| 2014  | GloVe                       |\n",
        "| 2017  | Transformers                |\n",
        "| 2018  | BERT                        |\n",
        "| 2019  | GPT-2                       |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Example of Word Complexity\n",
        "> \"Zuko makes the tea for his uncle.\"\n",
        "> \n",
        "> \"Zuko makes the coffee for his uncle.\"\n",
        "> \n",
        "> \"Zuko makes the drink for his uncle.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# One-hot Encoding Limitations\n",
        "- No notion of similarity between words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Problems with Discrete Properties\n",
        "- Reduced vocabulary\n",
        "- Sparse vectors\n",
        "- Always incomplete\n",
        "- Neural Networks prefer dense vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Advantages of Deep Learning Representations\n",
        "- Rich representations\n",
        "- Self-learning of complex objects"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Co-occurrence Matrices\n",
        "- Co-occurrence matrices are more useful than one-hot encoding.\n\n",
        "### Example:\n",
        "- tea\n",
        "- delicious\n",
        "- car\n",
        "- train\n",
        "- subway\n",
        "- kettle\n",
        "- teapot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Feedforward Neural Network Language Model (NNLM)\n\n",
        "Original Word2Vec Paper:\n",
        "- **Efficient Estimation of Word Representations in Vector Space**\n\n",
        "[Link to paper](https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes02-wordvecs2.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Recurrent Neural Network Language Model (RNNLM)\n\n",
        "Original Word2Vec Paper:\n",
        "- **Efficient Estimation of Word Representations in Vector Space**\n\n",
        "[Link to paper](https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes02-wordvecs2.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Continuous Bag-of-Words Model (CBOW)\n\n",
        "The CBOW model predicts a target word based on its surrounding context. The architecture can be summarized as:\n\n",
        "$$\\text{input} \\quad \\longrightarrow \\quad \\text{projection} \\quad \\longrightarrow \\quad \\text{sum} \\quad \\longrightarrow \\quad \\text{output}$$\n\n",
        "For example:\n",
        "- Words before the target: $ w(t-2), w(t-1) $\n",
        "- Words after the target: $ w(t+1), w(t+2) $\n",
        "- Predict target word $ w(t) $\n\n",
        "[Link to paper](https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes02-wordvecs2.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Continuous Skip-gram Model\n\n",
        "This model predicts surrounding words from a given target word.\n\n",
        "[Link to paper](https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes02-wordvecs2.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# GloVe: Global Vectors for Word Representation\n\n",
        "The GloVe model is based on the concept that the ratios of co-occurrence probabilities encode meaning in a vector space.\n\n",
        "[Link to paper](https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes02-wordvecs2.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Assignment Example: Exploring Word Vectors\n",
        "- **CS224N Assignment 1: Exploring Word Vectors**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
