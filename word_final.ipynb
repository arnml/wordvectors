{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d54d07c2-6221-4e7b-82ca-627f7394ef21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Introdução aos Word Vectors"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "### Word Vectors (Vetores de Palavras) são representações numéricas de palavras, geralmente na forma de vetores de alta dimensionalidade. Eles são usados para capturar o significado de palavras com base no seu contexto em um espaço vetorial. Esses vetores permitem que algoritmos de aprendizado de máquina compreendam relações semânticas entre palavras.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "display(Markdown(\"# Introdução aos Word Vectors\"))\n",
    "\n",
    "display(Markdown(\"\"\"\n",
    "### Word Vectors (Vetores de Palavras) são representações numéricas de palavras, geralmente na forma de vetores de alta dimensionalidade. Eles são usados para capturar o significado de palavras com base no seu contexto em um espaço vetorial. Esses vetores permitem que algoritmos de aprendizado de máquina compreendam relações semânticas entre palavras.\n",
    "\"\"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780e1a32",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50c592d5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d9cd1366-f917-4272-8aac-3caa99009d27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "### Os Word Vectors são fundamentais no campo do NLP porque permitem que máquinas compreendam e manipulem a linguagem de maneira mais semelhante ao humano.\n",
       "\n",
       "### - **Representação semântica**: Capturam o significado das palavras com base em seu uso.\n",
       "### - **Aplicações diversas**: Utilizados em tarefas como tradução automática, análise de sentimentos, e chatbots.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(\"\"\"\n",
    "### Os Word Vectors são fundamentais no campo do NLP porque permitem que máquinas compreendam e manipulem a linguagem de maneira mais semelhante ao humano.\n",
    "\n",
    "### - **Representação semântica**: Capturam o significado das palavras com base em seu uso.\n",
    "### - **Aplicações diversas**: Utilizados em tarefas como tradução automática, análise de sentimentos, e chatbots.\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042cb402",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d51f53b0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "148a0c41-0115-4295-8da3-3036e15049bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "## Exemplos de Modelos de Word Vectors\n",
       "### - **Word2Vec**: Um dos mais populares modelos para a criação de vetores de palavras, desenvolvido pelo Google.\n",
       "### - **GloVe (Global Vectors)**: Modelo criado pela Stanford.\n",
       "### - **FastText**: Desenvolvido pelo Facebook, é uma extensão do Word2Vec que trata palavras desconhecidas e derivadas (como plurais e conjugação).\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(\"\"\"\n",
    "## Exemplos de Modelos de Word Vectors\n",
    "### - **Word2Vec**: Um dos mais populares modelos para a criação de vetores de palavras, desenvolvido pelo Google.\n",
    "### - **GloVe (Global Vectors)**: Modelo criado pela Stanford.\n",
    "### - **FastText**: Desenvolvido pelo Facebook, é uma extensão do Word2Vec que trata palavras desconhecidas e derivadas (como plurais e conjugação).\n",
    "\"\"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39145886",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a67c46ca",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0ac7db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "## Como as Palavras são Representadas em Vetores\n",
       "### - A representação de palavras em vetores é uma forma de converter palavras, que são entidades discretas, em uma forma que os algoritmos de aprendizado de máquina possam processar — essencialmente, números.\n",
       "\n",
       "## 1. One-Hot Encoding\n",
       "### - One-Hot Encoding é uma forma simples de representar palavras, mas não captura semântica.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(\"\"\"\n",
    "## Como as Palavras são Representadas em Vetores\n",
    "### - A representação de palavras em vetores é uma forma de converter palavras, que são entidades discretas, em uma forma que os algoritmos de aprendizado de máquina possam processar — essencialmente, números.\n",
    "\n",
    "## 1. One-Hot Encoding\n",
    "### - One-Hot Encoding é uma forma simples de representar palavras, mas não captura semântica.\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0103dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palavra: homem -----> One-Hot: [1. 0. 0. 0.]\n",
      "\n",
      "Palavra: mulher -----> One-Hot: [0. 1. 0. 0.]\n",
      "\n",
      "Palavra: rainha -----> One-Hot: [0. 0. 1. 0.]\n",
      "\n",
      "Palavra: rei -----> One-Hot: [0. 0. 0. 1.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo de One-Hot Encoding com Sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Lista de palavras\n",
    "palavras = [['homem'], ['mulher'], ['rainha'], ['rei']]\n",
    "\n",
    "# Criando o One-Hot Encoder\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "one_hot = one_hot_encoder.fit_transform(palavras)\n",
    "\n",
    "# Exibindo as representações One-Hot\n",
    "for palavra, vetor in zip(palavras, one_hot):\n",
    "    print(f\"Palavra: {palavra[0]} -----> One-Hot: {vetor}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04df4009-68e2-4ef9-a15f-71bcf241453d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1b9d8d0-656b-4816-990e-9d84a3861e1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "## 2. Count Vectors\n",
       "### - Conta a frequência de cada palavra em um corpus.\n",
       "### - O CountVectorizer por padrão ignora palavras muito comuns, chamadas de \"stop words\" (como \"o\", \"a\", \"de\", etc.), porque elas geralmente não acrescentam significado importante em tarefas de processamento de linguagem natural.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(\"\"\"\n",
    "## 2. Count Vectors\n",
    "### - Conta a frequência de cada palavra em um corpus.\n",
    "### - O CountVectorizer por padrão ignora palavras muito comuns, chamadas de \"stop words\" (como \"o\", \"a\", \"de\", etc.), porque elas geralmente não acrescentam significado importante em tarefas de processamento de linguagem natural.\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84d66330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulário: ['cachorro' 'gato' 'late' 'mia']\n",
      "\n",
      "Count Vectors:\n",
      " [[1 0 1 0]\n",
      " [0 1 0 1]\n",
      " [1 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# Exemplo de Count Vectors com Sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Corpus de exemplo\n",
    "corpus = [\n",
    "    'o cachorro late',\n",
    "    'o gato mia',\n",
    "    'o cachorro mia',\n",
    "]\n",
    "\n",
    "# Criando o Count Vectorizer\n",
    "vectorizer = CountVectorizer() \n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Exibindo a representação das palavras\n",
    "print(\"Vocabulário:\", vectorizer.get_feature_names_out())\n",
    "print(\"\\nCount Vectors:\\n\", X.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a2307e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "685c6df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "## TF-IDF (Term Frequency - Inverse Document Frequency)\n",
       "### - Atribui pesos baseados na frequência das palavras, mas também ajusta os pesos através da raridade delas.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(\"\"\"\n",
    "## TF-IDF (Term Frequency - Inverse Document Frequency)\n",
    "### - Atribui pesos baseados na frequência das palavras, mas também ajusta os pesos através da raridade delas.\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9492ebfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulário: ['cachorro' 'gato' 'late' 'mia']\n",
      "\n",
      "TF-IDF Vectors:\n",
      " [[0.55645052 0.         0.83088075 0.        ]\n",
      " [0.         0.83088075 0.         0.55645052]\n",
      " [0.70710678 0.         0.         0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "# Exemplo de TF-IDF com Sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Corpus de exemplo\n",
    "corpus = [\n",
    "    'o cachorro late',\n",
    "    'o gato mia',\n",
    "    'o cachorro mia',\n",
    "]\n",
    "\n",
    "# Criando o TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer(smooth_idf=False)\n",
    "X = tfidf.fit_transform(corpus)\n",
    "\n",
    "# Exibindo a representação das palavras\n",
    "print(\"Vocabulário:\", tfidf.get_feature_names_out())\n",
    "print(\"\\nTF-IDF Vectors:\\n\", X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23f43c1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "### - Frequência do Termo (TF) = Número de vezes que a palavra desejada aparece na frase / Número total de frases.\n",
       "### - Frequência Inversa do Documento (IDF) = log ( Número total de frases / Número de documentos que contem a palvra desejada )\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(\"\"\"\n",
    "### - Frequência do Termo (TF) = Número de vezes que a palavra desejada aparece na frase / Número total de frases.\n",
    "### - Frequência Inversa do Documento (IDF) = log ( Número total de frases / Número de documentos que contem a palvra desejada )\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2b35fc6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "## corpus = [\n",
       "##    'o cachorro late',\n",
       "##    'o gato mia',\n",
       "##    'o cachorro mia'\n",
       "## ]\n",
       "\n",
       "### - TF(cachorro) = 1/3 = 0.333\n",
       "### - IDF(cachorro) = log(3/2) ≈ 0.176\n",
       "\n",
       "### - TF × IDF = 1/3 * log(3/2) = 0.05869708635 \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(\"\"\"\n",
    "## corpus = [\n",
    "##    'o cachorro late',\n",
    "##    'o gato mia',\n",
    "##    'o cachorro mia'\n",
    "## ]\n",
    "\n",
    "### - TF(cachorro) = 1/3 = 0.333\n",
    "### - IDF(cachorro) = log(3/2) ≈ 0.176\n",
    "\n",
    "### - TF × IDF = 1/3 * log(3/2) = 0.05869708635 \n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f72c9a9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e9644c4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "163b5b1d-0b32-4910-8a45-4701a7ffbe3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "## O que é Bag of Words?\n",
       "### - O Bag of Words é uma técnica simples para representar texto numericamente. Ele transforma um conjunto de documentos em uma matriz de frequências, onde cada linha representa um documento e cada coluna representa uma palavra do vocabulário total. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(\"\"\"\n",
    "## O que é Bag of Words?\n",
    "### - O Bag of Words é uma técnica simples para representar texto numericamente. Ele transforma um conjunto de documentos em uma matriz de frequências, onde cada linha representa um documento e cada coluna representa uma palavra do vocabulário total. \n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7dd7b9e3-b019-4831-ab07-257efa8c1c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "## Construção do Vocabulário\n",
       "### - Para construir o BoW, primeiro, precisamos identificar todas as palavras únicas (ou tokens) em um corpus de texto. Isso forma o vocabulário do modelo.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(\"\"\"\n",
    "## Construção do Vocabulário\n",
    "### - Para construir o BoW, primeiro, precisamos identificar todas as palavras únicas (ou tokens) em um corpus de texto. Isso forma o vocabulário do modelo.\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b67f592-766f-4b80-a190-10c336cc3f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulário: ['alto' 'cachorro' 'estão' 'gato' 'juntos' 'late' 'mia' 'suavemente']\n"
     ]
    }
   ],
   "source": [
    "# Exemplo prático - Construção do Vocabulário\n",
    "from sklearn.feature_extraction.text import CountVectorizer  \n",
    "\n",
    "# Lista de documentos de exemplo\n",
    "corpus = [\n",
    "    'O cachorro late alto',\n",
    "    'O gato mia suavemente',\n",
    "    'O cachorro e o gato estão juntos'\n",
    "]\n",
    "\n",
    "# Criando o modelo BoW\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Exibindo o vocabulário criado\n",
    "print(\"Vocabulário:\", vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f85bccf3-41a5-4093-ba12-441d3465265f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "## Representação dos Documentos com Frequência de Palavras\n",
       "### - Depois de construir o vocabulário, cada documento é representado como um vetor de frequências, onde cada posição no vetor indica o número de vezes que a palavra correspondente apareceu no documento..\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(\"\"\"\n",
    "## Representação dos Documentos com Frequência de Palavras\n",
    "### - Depois de construir o vocabulário, cada documento é representado como um vetor de frequências, onde cada posição no vetor indica o número de vezes que a palavra correspondente apareceu no documento..\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f8b6424-966f-45dc-bdaf-b4a57eb68971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matriz BoW:\n",
      " [[1 1 0 0 0 1 0 0]\n",
      " [0 0 0 1 0 0 1 1]\n",
      " [0 1 1 1 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Exemplo prático - Representação dos Documentos\n",
    "# Convertendo os documentos para a matriz BoW\n",
    "print(\"\\nMatriz BoW:\\n\", X.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9e9c5f42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "## Pré-processamento do Texto:\n",
       "\n",
       "### - Normalização: Muitas vezes, é útil normalizar o texto, como converter tudo para minúsculas ou remover pontuação.\n",
       "### - Stop Words: Pode ser útil remover palavras comuns (como \"o\", \"a\", \"e\", etc.) que não agregam significado relevante.\n",
       "### - Stemming/Lemmatization: Reduzir as palavras às suas raízes pode ajudar a reduzir a dimensionalidade do vocabulário. Ex: amigos para amig\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(\"\"\"\n",
    "## Pré-processamento do Texto:\n",
    "\n",
    "### - Normalização: Muitas vezes, é útil normalizar o texto, como converter tudo para minúsculas ou remover pontuação.\n",
    "### - Stop Words: Pode ser útil remover palavras comuns (como \"o\", \"a\", \"e\", etc.) que não agregam significado relevante.\n",
    "### - Stemming/Lemmatization: Reduzir as palavras às suas raízes pode ajudar a reduzir a dimensionalidade do vocabulário. Ex: amigos para amig\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8b6c1b02-f494-4f7c-9f03-265fc171594f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "## Limitações do Bag of Words\n",
       "\n",
       "### - Ignora a Ordem das Palavras: BoW não considera a ordem em que as palavras aparecem, o que pode resultar em perda de contexto.\n",
       "### - Sparsidade da Matriz: A matriz resultante é frequentemente esparsa, com muitas posições zeradas, especialmente se o vocabulário for grande.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(\"\"\"\n",
    "## Limitações do Bag of Words\n",
    "\n",
    "### - Ignora a Ordem das Palavras: BoW não considera a ordem em que as palavras aparecem, o que pode resultar em perda de contexto.\n",
    "### - Sparsidade da Matriz: A matriz resultante é frequentemente esparsa, com muitas posições zeradas, especialmente se o vocabulário for grande.\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53edf35c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "## Exemplo de uso do Bag of words para análise de sentimentos\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(\"\"\"\n",
    "## Exemplo de uso do Bag of words para análise de sentimentos\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e96e512a-ddf3-4220-b71b-ac0cebf1647b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabulário: ['adorei' 'antes' 'anunciado' 'ao' 'aqui' 'bom' 'chegou' 'coisa' 'com'\n",
      " 'compra' 'comprar' 'compraria' 'comprei' 'corresponde' 'cumpre' 'deles'\n",
      " 'do' 'essa' 'esse' 'estou' 'eu' 'excelente' 'expectativas' 'foi' 'frágil'\n",
      " 'funciona' 'horrível' 'mais' 'maravilhoso' 'minha' 'minhas' 'muito' 'na'\n",
      " 'ninguém' 'nossa' 'não' 'perfeitamente' 'pior' 'prazo' 'produto'\n",
      " 'prometido' 'péssima' 'qualidade' 'que' 'quebrado' 'recomendo' 'ruim'\n",
      " 'satisfeito' 'superou' 'vida' 'voltarei']\n",
      "\n",
      "Matriz BoW:\n",
      " [[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 1 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1\n",
      "  0 0 0 1 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0\n",
      "  1 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1\n",
      "  0 0 0 1 0 0 0 0 0 1 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 1 0 0 1]\n",
      " [0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      "  0 0 0 0 0 1 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1\n",
      "  0 0 0 1 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0\n",
      "  0 0 0 1 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1\n",
      "  0 1 0 0 0 0 0 1 0 1 0 0 0 1 0]]\n",
      "\n",
      "Saida do algoritimo: [1]\n",
      "\n",
      "Frase: \n",
      "O produto é maravilhoso e funciona muito bem, muito mais do que eu esperava, estou satisfeito, compraria novamente  - Predição: Positivo\n"
     ]
    }
   ],
   "source": [
    "# Importando bibliotecas necessárias\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Simulação de um dataset de reviews em português\n",
    "corpus = [\n",
    "    'Esse produto é excelente, adorei a qualidade',\n",
    "    'Horrível, o produto chegou quebrado e não funciona',\n",
    "    'Muito bom, chegou antes do prazo e funciona perfeitamente',\n",
    "    'Produto ruim, não recomendo a ninguém',\n",
    "    'Estou muito satisfeito com minha compra, voltarei a comprar aqui',\n",
    "    'Péssima qualidade, não corresponde ao anunciado',\n",
    "    'Maravilhoso, superou minhas expectativas',\n",
    "    'O produto é frágil e não cumpre o prometido',\n",
    "    'Nossa que produto bom, eu compraria muito mais deles',\n",
    "    'Essa foi a pior coisa que eu comprei na minha vida, não recomendo'\n",
    "]\n",
    "\n",
    "# Rótulos: 1 = positivo, 0 = negativo\n",
    "labels = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
    "\n",
    "# Transformando o corpus em Bag of Words\n",
    "vectorizer = CountVectorizer() \n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(\"\\nVocabulário:\", vectorizer.get_feature_names_out())\n",
    "print(\"\\nMatriz BoW:\\n\", X.toarray())\n",
    "\n",
    "# Dividindo os dados em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.4)\n",
    "\n",
    "# Treinando o classificador Naive Bayes\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Função para prever o sentimento de uma nova frase\n",
    "def prever_sentimento(frase):\n",
    "    frase_transformada = vectorizer.transform([frase])  # Transforma a frase usando o mesmo vectorizer\n",
    "    predicao = clf.predict(frase_transformada)  # Faz a predição\n",
    "    print(\"\\nSaida do algoritimo:\", predicao)\n",
    "    return \"Positivo\" if predicao[0] == 1 else \"Negativo\"\n",
    "\n",
    "# Exemplo de uso da função\n",
    "nova_frase = \"\\nO produto é maravilhoso e funciona muito bem, muito mais do que eu esperava, estou satisfeito, compraria novamente\"\n",
    "resultado = prever_sentimento(nova_frase)\n",
    "print(\"\\nFrase:\", nova_frase, \" - Predição:\", resultado)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ba04f9-a67e-4e4c-99e2-f3344b72070b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eab9f1fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "## Word2Vec é uma técnica poderosa para gerar representações densas de palavras (word embeddings) que capturam relações semânticas e sintáticas entre palavras. Essas representações são amplamente utilizadas em diversas aplicações de Processamento de Linguagem Natural (NLP). Existem duas principais arquiteturas para treinar o Word2Vec:\n",
       "\n",
       "### 1. **Continuous Bag of Words (CBOW)**: Prediz uma palavra alvo com base no contexto de palavras vizinhas.\n",
       "### 2. **Skip-gram**: Prediz palavras de contexto com base em uma palavra alvo.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(\"\"\"\n",
    "## Word2Vec é uma técnica poderosa para gerar representações densas de palavras (word embeddings) que capturam relações semânticas e sintáticas entre palavras. Essas representações são amplamente utilizadas em diversas aplicações de Processamento de Linguagem Natural (NLP). Existem duas principais arquiteturas para treinar o Word2Vec:\n",
    "\n",
    "### 1. **Continuous Bag of Words (CBOW)**: Prediz uma palavra alvo com base no contexto de palavras vizinhas.\n",
    "### 2. **Skip-gram**: Prediz palavras de contexto com base em uma palavra alvo.\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d9d54ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Continuous Bag of Words (CBOW)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "### O CBOW tenta prever uma palavra alvo usando as palavras do contexto ao redor. Por exemplo, na frase \"o cachorro está latindo\", se a palavra alvo for \"cachorro\", o modelo CBOW tenta prever \"cachorro\" usando \"o\", \"está\", e \"latindo\".\n",
       "\n",
       "## **Como Funciona:**\n",
       "### 1. **Entrada**: Primeiro, representamos as palavras de contexto usando vetores One-Hot. Um vetor One-Hot é um vetor binário que possui um 1 na posição da palavra no vocabulário e 0 em todas as outras posições.\n",
       "\n",
       "Vocabulário:\n",
       "\n",
       "    \"o\" → [1, 0, 0, 0, 0]\n",
       "    \"cachorro\" → [0, 1, 0, 0, 0]\n",
       "    \"está\" → [0, 0, 1, 0, 0]\n",
       "    \"latindo\" → [0, 0, 0, 1, 0]\n",
       "\n",
       "\n",
       "Vetores de Contexto:\n",
       "\n",
       "    \"o\" → [1, 0, 0, 0, 0]\n",
       "    \"está\" → [0, 0, 1, 0, 0]\n",
       "    \"latindo\" → [0, 0, 0, 1, 0]    \n",
       "\n",
       "### 2. **Camada Oculta**: A camada oculta combina os vetores de contexto para formar uma representação do contexto. Normalmente, faz isso calculando a média dos vetores One-Hot.\n",
       "\n",
       "### Cálculo da Média:\n",
       "### h= 1/3 ( [1,0,0,0,0] + [0,0,1,0,0] + [0,0,0,1,0] )= [0.33,0,0.33,0.33,0]\n",
       "\n",
       "### 3. **Saída**: Função de Saída: CBOW geralmente usa softmax na camada de saída.\n",
       "### Como Funciona: CBOW prevê a palavra alvo usando as palavras de contexto ao redor. Por exemplo, na frase \"o gato está dormindo\", CBOW usaria as palavras \"o\", \"está\", e \"dormindo\" para prever \"gato\".\n",
       "### Uso do Softmax: A função softmax é usada para calcular a probabilidade de cada palavra no vocabulário ser a palavra alvo, considerando o contexto. A softmax é aplicada na última camada da rede para transformar as pontuações dos embeddings em probabilidades, facilitando a previsão da palavra correta.\n",
       "\n",
       "### Resumidante Softmax é usada para múltiplas classes, transformando um vetor de valores em probabilidades que somam 1.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(\"# Continuous Bag of Words (CBOW)\"))\n",
    "display(Markdown(\"\"\"\n",
    "### O CBOW tenta prever uma palavra alvo usando as palavras do contexto ao redor. Por exemplo, na frase \"o cachorro está latindo\", se a palavra alvo for \"cachorro\", o modelo CBOW tenta prever \"cachorro\" usando \"o\", \"está\", e \"latindo\".\n",
    "\n",
    "## **Como Funciona:**\n",
    "### 1. **Entrada**: Primeiro, representamos as palavras de contexto usando vetores One-Hot. Um vetor One-Hot é um vetor binário que possui um 1 na posição da palavra no vocabulário e 0 em todas as outras posições.\n",
    "\n",
    "Vocabulário:\n",
    "\n",
    "    \"o\" → [1, 0, 0, 0, 0]\n",
    "    \"cachorro\" → [0, 1, 0, 0, 0]\n",
    "    \"está\" → [0, 0, 1, 0, 0]\n",
    "    \"latindo\" → [0, 0, 0, 1, 0]\n",
    "\n",
    "\n",
    "Vetores de Contexto:\n",
    "\n",
    "    \"o\" → [1, 0, 0, 0, 0]\n",
    "    \"está\" → [0, 0, 1, 0, 0]\n",
    "    \"latindo\" → [0, 0, 0, 1, 0]    \n",
    "\n",
    "### 2. **Camada Oculta**: A camada oculta combina os vetores de contexto para formar uma representação do contexto. Normalmente, faz isso calculando a média dos vetores One-Hot.\n",
    "\n",
    "### Cálculo da Média:\n",
    "### h= 1/3 ( [1,0,0,0,0] + [0,0,1,0,0] + [0,0,0,1,0] )= [0.33,0,0.33,0.33,0]\n",
    "\n",
    "### 3. **Saída**: Função de Saída: CBOW geralmente usa softmax na camada de saída.\n",
    "### Como Funciona: CBOW prevê a palavra alvo usando as palavras de contexto ao redor. Por exemplo, na frase \"o gato está dormindo\", CBOW usaria as palavras \"o\", \"está\", e \"dormindo\" para prever \"gato\".\n",
    "### Uso do Softmax: A função softmax é usada para calcular a probabilidade de cada palavra no vocabulário ser a palavra alvo, considerando o contexto. A softmax é aplicada na última camada da rede para transformar as pontuações dos embeddings em probabilidades, facilitando a previsão da palavra correta.\n",
    "\n",
    "### Resumidante Softmax é usada para múltiplas classes, transformando um vetor de valores em probabilidades que somam 1.\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2647f68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vetor da palavra 'cachorro' usando CBOW:\n",
      " [-0.08801845  0.06749935 -0.04983679  0.02011902  0.06611765 -0.07721087\n",
      "  0.08120681 -0.03733265 -0.08021297  0.07045902]\n",
      "\n",
      "Palavras mais similares a 'cachorro': [('dormindo', 0.6484286785125732), ('cantando', 0.5246529579162598), ('gato', 0.4149939715862274)]\n"
     ]
    }
   ],
   "source": [
    "# Importando a biblioteca necessária\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Corpus de exemplo\n",
    "sentencas = [\n",
    "    ['o', 'gato', 'está', 'dormindo'],\n",
    "    ['o', 'cachorro', 'está', 'latindo'],\n",
    "    ['o', 'passaro', 'está', 'cantando']\n",
    "]\n",
    "\n",
    "# Treinando o modelo CBOW\n",
    "# sg=0 indica CBOW (se fosse sg=1, seria Skip-gram)\n",
    "model_cbow = Word2Vec(sentencas, vector_size=10, window=2, min_count=1, sg=0, epochs=1000)\n",
    "\n",
    "# Exibindo o vetor da palavra 'cachorro'\n",
    "print(\"Vetor da palavra 'cachorro' usando CBOW:\\n\", model_cbow.wv['gato'])\n",
    "\n",
    "# Predição: encontrando palavras similares à palavra 'cachorro'\n",
    "similar_words = model_cbow.wv.most_similar(['o', 'está', 'dormindo'], topn=3)\n",
    "print(\"\\nPalavras mais similares a 'cachorro':\", similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "675598b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Skip-gram"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "### O Skip-gram faz o oposto do CBOW: ele usa a palavra alvo para prever as palavras de contexto. Exemplo: Na frase \"o gato está dormindo\", a palavra \"gato\" é usada para prever \"o\", \"está\", e \"dormindo\".\n",
       "\n",
       "## **Como Funciona:**\n",
       "### 1. **Entrada**: Vetor One-Hot da palavra alvo.\n",
       "### 2. **Camada Oculta**: Projeta o vetor da palavra alvo em um espaço de embedding.\n",
       "### 3. **Saída**: Usa sigmoid e Negative Sampling para prever as palavras de contexto.\n",
       "### 4. **Sigmoid** é ideal para previsões binárias e retorna uma probabilidade entre 0 e 1.\n",
       "\n",
       "## Negative Sampling:\n",
       "### Em vez de calcular o softmax sobre todas as palavras, o Skip-gram com Negative Sampling reduz o custo amostrando algumas \"palavras negativas\" que não estão no contexto.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(\"# Skip-gram\"))\n",
    "display(Markdown(\"\"\"\n",
    "### O Skip-gram faz o oposto do CBOW: ele usa a palavra alvo para prever as palavras de contexto. Exemplo: Na frase \"o gato está dormindo\", a palavra \"gato\" é usada para prever \"o\", \"está\", e \"dormindo\".\n",
    "\n",
    "## **Como Funciona:**\n",
    "### 1. **Entrada**: Vetor One-Hot da palavra alvo.\n",
    "### 2. **Camada Oculta**: Projeta o vetor da palavra alvo em um espaço de embedding.\n",
    "### 3. **Saída**: Usa sigmoid e Negative Sampling para prever as palavras de contexto.\n",
    "### 4. **Sigmoid** é ideal para previsões binárias e retorna uma probabilidade entre 0 e 1.\n",
    "\n",
    "## Negative Sampling:\n",
    "### Em vez de calcular o softmax sobre todas as palavras, o Skip-gram com Negative Sampling reduz o custo amostrando algumas \"palavras negativas\" que não estão no contexto.\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52c49867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vetor da palavra 'cachorro' usando Skip-gram:\n",
      " [-0.08384467  0.05030638 -0.04386335  0.01058092  0.08159344 -0.05159701\n",
      "  0.05148339 -0.06023112 -0.04447963  0.09067195]\n",
      "\n",
      "Palavras mais similares a 'gato' usando Skip-gram: [('latindo', 0.24998393654823303), ('pássaro', 0.10116135329008102), ('gato', 0.004566082265228033)]\n"
     ]
    }
   ],
   "source": [
    "# Importando a biblioteca necessária\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Corpus de exemplo\n",
    "sentencas = [\n",
    "    ['o', 'gato', 'está', 'dormindo'],\n",
    "    ['o', 'cachorro', 'está', 'latindo'],\n",
    "    ['o', 'pássaro', 'está', 'cantando']\n",
    "]\n",
    "\n",
    "# Treinando o modelo Skip-gram\n",
    "# sg=1 indica Skip-gram (se fosse sg=0, seria CBOW)\n",
    "model_skipgram = Word2Vec(sentencas, vector_size=10, window=2, min_count=1, sg=1, epochs=1000)\n",
    "\n",
    "# Exibindo o vetor da palavra 'cachorro'\n",
    "print(\"Vetor da palavra 'cachorro' usando Skip-gram:\\n\", model_skipgram.wv['cachorro'])\n",
    "\n",
    "# Predição: encontrando palavras similares à palavra 'gato'\n",
    "similar_words_skipgram = model_skipgram.wv.most_similar('cachorro', topn=3)\n",
    "print(\"\\nPalavras mais similares a 'gato' usando Skip-gram:\", similar_words_skipgram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049150a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf87f84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644b485e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
